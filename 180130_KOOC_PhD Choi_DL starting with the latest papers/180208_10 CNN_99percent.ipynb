{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-Slim 을 이용하여 Mnist dataset을 99%이상 분류하는 CNN 모델을 만들자.\n",
    "# Batch normalization, Data argumentation\n",
    "\n",
    "# 이활성 님이 만든 코드를 바탕으로 만들었음.\n",
    "# https://github.com/hwalsuklee/tensorflow-mnist-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PACKAGES LOADED\n",
      "Tensorflow version is 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import os\n",
    "from scipy import ndimage\n",
    "from six.moves import urllib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "print (\"PACKAGES LOADED\")\n",
    "print (\"Tensorflow version is %s\" % (tf.__version__))\n",
    "\n",
    "# 텐서플로 버전 1.1부터는 TF-Slim이 들어가 있다.\n",
    "# as slim으로 모듈을 추가하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN MODEL WITH TF-SLIM\n",
    "# Batch normalization을 쓸 것.\n",
    "# 그런데 BN으로 트레이닝 할 때는 현재 mean과 variance을 계산을 해서 빼준다.\n",
    "# 매번 들어올 때마다 mean과 variance가 다르다.\n",
    "# 그런데 테스트 할 때는 어떻게 할까?\n",
    "# 어떤 하나의 mean과 variance를 써야 한다.\n",
    "# 즉, Batch normalization에서 트레이닝 할 때는 \n",
    "# mean과 variance가 들어오면 EMA(Exponential Moving Average)를 계산해서 가져온다.\n",
    "# 테스트 할 때는 계속 업데이트 되어지다 트레이닝이 끝날 당시에 구해진 EMA로 얻어진\n",
    "# mean과 variance를 가지고 지금 들어온 mini Batch를 normalize 한다.\n",
    "# 그래서 트레이닝 할 곳과 테스트 할 곳을 따로 만들어 주어야 한다.\n",
    "# EMA이 업데이트도 해주어야 하기 때문에 구현이 어렵다.\n",
    "## -> 이 것을 TF-Slim이 해준다!!\n",
    "\n",
    "def CNN(inputs, _is_training=True):\n",
    "    # 텐서플로 reshape 라이브러리 사용\n",
    "    # -1은 몇개의 입력데이터가 들어올지 모르겠다는 표현\n",
    "    # 28x28 크기의 흑백 이미지로 reshape\n",
    "    x   = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "    batch_norm_params = {'is_training': _is_training, 'decay': 0.9, 'updates_collections': None}\n",
    "    # TF-Slim용 라이브러리 사용\n",
    "    # 컨볼루션 - 맥스풀링 2층 사용\n",
    "    net = slim.conv2d(x, 32, [5, 5], padding='SAME'\n",
    "                     , activation_fn       = tf.nn.relu\n",
    "                     , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                     , normalizer_fn       = slim.batch_norm\n",
    "                     , normalizer_params   = batch_norm_params\n",
    "                     , scope='conv1')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "    \n",
    "    net = slim.conv2d(net, 64, [5, 5], scope='conv2')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "    \n",
    "    # flatten : 한줄로 펴주는 작업\n",
    "    net = slim.flatten(net, scope='flatten3')\n",
    "    \n",
    "    # FC layer\n",
    "    net = slim.fully_connected(net, 1024\n",
    "                    , activation_fn       = tf.nn.relu\n",
    "                    , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                    , normalizer_fn       = slim.batch_norm\n",
    "                    , normalizer_params   = batch_norm_params\n",
    "                    , scope='fc4')\n",
    "    \n",
    "    # dropout\n",
    "    net = slim.dropout(net, keep_prob=0.7, is_training=_is_training, scope='dropout4')  \n",
    "    out = slim.fully_connected(net, 10, activation_fn=None, normalizer_fn=None, scope='fco')\n",
    "    return out\n",
    "\n",
    "\n",
    "# 정말 간단하게 구현된다.\n",
    "# weight를 따로 지정해줄 필요가 없음.\n",
    "# slim과 비슷하게 캐나스도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HANDLING MNIST\n",
    "# MNIST 데이터를 받고 argumentation을 한다.\n",
    "\n",
    "# DATA URL\n",
    "SOURCE_URL      = 'http://yann.lecun.com/exdb/mnist/'\n",
    "DATA_DIRECTORY  = \"data\"\n",
    "# PARAMETERS FOR MNIST\n",
    "IMAGE_SIZE      = 28\n",
    "NUM_CHANNELS    = 1\n",
    "PIXEL_DEPTH     = 255\n",
    "NUM_LABELS      = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "\n",
    "# 다운로드를 하네\n",
    "# DOWNLOAD MNIST DATA, IF NECESSARY\n",
    "def maybe_download(filename):\n",
    "    if not tf.gfile.Exists(DATA_DIRECTORY):\n",
    "        tf.gfile.MakeDirs(DATA_DIRECTORY)\n",
    "    filepath = os.path.join(DATA_DIRECTORY, filename)\n",
    "    if not tf.gfile.Exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        with tf.gfile.GFile(filepath) as f:\n",
    "            size = f.size()\n",
    "        print('Successfully downloaded', filename, size, 'bytes.')\n",
    "    return filepath\n",
    "\n",
    "# EXTRACT IMAGES\n",
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH # -0.5~0.5\n",
    "        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "        data = np.reshape(data, [num_images, -1])\n",
    "    return data # [image index, y, x, channels]\n",
    "\n",
    "# EXTRACT LABELS\n",
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        num_labels_data = len(labels)\n",
    "        one_hot_encoding = np.zeros((num_labels_data,NUM_LABELS))\n",
    "        one_hot_encoding[np.arange(num_labels_data),labels] = 1\n",
    "        one_hot_encoding = np.reshape(one_hot_encoding, [-1, NUM_LABELS])\n",
    "    return one_hot_encoding\n",
    "\n",
    "# 내가 가지고 있는 트레이닝 데이터의 수를 뻥튀기 해주는 작업\n",
    "# 로테이션 쉬프트 & 랜덤 쉬프트를 한다.\n",
    "# AUGMENT TRAINING DATA\n",
    "def expend_training_data(images, labels):\n",
    "    expanded_images = []\n",
    "    expanded_labels = []\n",
    "    j = 0 # counter\n",
    "    for x, y in zip(images, labels):\n",
    "        j = j+1\n",
    "        # APPEND ORIGINAL DATA\n",
    "        expanded_images.append(x)\n",
    "        expanded_labels.append(y)\n",
    "        # ASSUME MEDIAN COLOR TO BE BACKGROUND COLOR\n",
    "        bg_value = np.median(x) # this is regarded as background's value        \n",
    "        image = np.reshape(x, (-1, 28))\n",
    "\n",
    "        for i in range(4):\n",
    "            # ROTATE IMAGE\n",
    "            angle = np.random.randint(-15,15,1)\n",
    "            new_img = ndimage.rotate(image,angle,reshape=False, cval=bg_value)\n",
    "            # SHIFT IAMGE\n",
    "            shift = np.random.randint(-2, 2, 2)\n",
    "            new_img_ = ndimage.shift(new_img,shift, cval=bg_value)\n",
    "            # ADD TO THE LIST\n",
    "            expanded_images.append(np.reshape(new_img_, 784))\n",
    "            expanded_labels.append(y)\n",
    "    expanded_train_total_data = np.concatenate((expanded_images, expanded_labels), axis=1)\n",
    "    np.random.shuffle(expanded_train_total_data)\n",
    "    return expanded_train_total_data\n",
    "\n",
    "# PREPARE MNIST DATA\n",
    "def prepare_MNIST_data(use_data_augmentation=True):\n",
    "    # Get the data.\n",
    "    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "    train_data = extract_data(train_data_filename, 60000)\n",
    "    train_labels = extract_labels(train_labels_filename, 60000)\n",
    "    test_data = extract_data(test_data_filename, 10000)\n",
    "    test_labels = extract_labels(test_labels_filename, 10000)\n",
    "    validation_data = train_data[:VALIDATION_SIZE, :]\n",
    "    validation_labels = train_labels[:VALIDATION_SIZE,:]\n",
    "    train_data = train_data[VALIDATION_SIZE:, :]\n",
    "    train_labels = train_labels[VALIDATION_SIZE:,:]\n",
    "    if use_data_augmentation:\n",
    "        train_total_data = expend_training_data(train_data, train_labels)\n",
    "    else:\n",
    "        train_total_data = np.concatenate((train_data, train_labels), axis=1)\n",
    "    train_size = train_total_data.shape[0]\n",
    "    return train_total_data, train_size, validation_data, validation_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "MODEL_DIRECTORY   = \"model/model.ckpt\"\n",
    "LOGS_DIRECTORY    = \"logs/train\"\n",
    "training_epochs   = 10\n",
    "TRAIN_BATCH_SIZE  = 50\n",
    "display_step      = 500\n",
    "validation_step   = 500\n",
    "TEST_BATCH_SIZE   = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREPARE MNIST DATA\n",
    "# 트레이닝 데이터가 55000개에서 275000개가 되었음.\n",
    "\n",
    "batch_size = TRAIN_BATCH_SIZE # BATCH SIZE (50)\n",
    "num_labels = NUM_LABELS       # NUMBER OF LABELS (10)\n",
    "train_total_data, train_size, validation_data, validation_labels \\\n",
    "    , test_data, test_labels = prepare_MNIST_data(True)\n",
    "# PRINT FUNCTION\n",
    "def print_np(x, str):\n",
    "    print (\" TYPE AND SHAPE OF [%18s ] ARE %s and %14s\" \n",
    "           % (str, type(x), x.shape,))\n",
    "print_np(train_total_data, 'train_total_data')\n",
    "print_np(validation_data, 'validation_data')\n",
    "print_np(validation_labels, 'validation_labels')\n",
    "print_np(test_data, 'test_data')\n",
    "print_np(test_labels, 'test_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINE MODEL\n",
    "\n",
    "# PLACEHOLDERS\n",
    "x  = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) #answer\n",
    "is_training = tf.placeholder(tf.bool, name='MODE')\n",
    "# CONVOLUTIONAL NEURAL NETWORK MODEL \n",
    "y = CNN(x, is_training)\n",
    "# DEFINE LOSS\n",
    "with tf.name_scope(\"LOSS\"):\n",
    "    loss = slim.losses.softmax_cross_entropy(y, y_)\n",
    "# DEFINE ACCURACY\n",
    "with tf.name_scope(\"ACC\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# DEFINE OPTIMIZER\n",
    "with tf.name_scope(\"ADAM\"):\n",
    "    batch = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        1e-4,               # LEARNING_RATE\n",
    "        batch * batch_size, # GLOBAL_STEP\n",
    "        train_size,         # DECAY_STEP\n",
    "        0.95,               # DECAY_RATE\n",
    "        staircase=True)     # LR = LEARNING_RATE*DECAY_RATE^(GLOBAL_STEP/DECAY_STEP)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss,global_step=batch)\n",
    "    # 'batch' IS AUTOMATICALLY UPDATED AS WE CALL 'train_step'\n",
    "\n",
    "# SUMMARIES\n",
    "saver = tf.train.Saver()\n",
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "tf.summary.scalar('loss', loss)\n",
    "tf.summary.scalar('acc', accuracy)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(LOGS_DIRECTORY, graph=tf.get_default_graph())\n",
    "print (\"MODEL DEFINED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPEN SESSION\n",
    "\n",
    "sess  = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer(), feed_dict={is_training: True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIMIZE\n",
    "# FOR TESTING PURPOSES, SKIP THIS SECTION\n",
    "\n",
    "# MAXIMUM ACCURACY\n",
    "max_acc = 0.\n",
    "# LOOP\n",
    "for epoch in range(training_epochs): # training_epochs: 10\n",
    "    # RANDOM SHUFFLE\n",
    "    np.random.shuffle(train_total_data)\n",
    "    train_data_   = train_total_data[:, :-num_labels]\n",
    "    train_labels_ = train_total_data[:, -num_labels:]\n",
    "    # ITERATIONS\n",
    "    total_batch = int(train_size / batch_size)\n",
    "    for iteration in range(total_batch):\n",
    "        # GET CURRENT MINI-BATCH\n",
    "        offset = (iteration * batch_size) % (train_size)\n",
    "        batch_xs = train_data_[offset:(offset + batch_size), :]\n",
    "        batch_ys = train_labels_[offset:(offset + batch_size), :]\n",
    "        # OPTIMIZE\n",
    "        _, train_accuracy, summary = sess.run([train_step, accuracy, merged_summary_op]\n",
    "                                    , feed_dict={x: batch_xs, y_: batch_ys, is_training: True})\n",
    "        # WRITE LOG\n",
    "        summary_writer.add_summary(summary, epoch*total_batch + iteration)\n",
    "\n",
    "        # DISPLAY\n",
    "        if iteration % display_step == 0:\n",
    "            print(\"Epoch: [%3d/%3d] Batch: [%04d/%04d] Training Acc: %.5f\" \n",
    "                  % (epoch + 1, training_epochs, iteration, total_batch, train_accuracy))\n",
    "\n",
    "        # GET ACCURACY FOR THE VALIDATION DATA\n",
    "        if iteration % validation_step == 0:\n",
    "            validation_accuracy = sess.run(accuracy,\n",
    "            feed_dict={x: validation_data, y_: validation_labels, is_training: False})\n",
    "            print(\"Epoch: [%3d/%3d] Batch: [%04d/%04d] Validation Acc: %.5f\" \n",
    "                  % (epoch + 1, training_epochs, iteration, total_batch, validation_accuracy))\n",
    "        # SAVE THE MODEL WITH HIGEST VALIDATION ACCURACY\n",
    "        if validation_accuracy > max_acc:\n",
    "            # 검증된 정확도가 제일 높을 때만 저장\n",
    "            max_acc = validation_accuracy\n",
    "            save_path = saver.save(sess, MODEL_DIRECTORY)\n",
    "            print(\"  MODEL UPDATED TO [%s] VALIDATION ACC IS %.5f\" \n",
    "                  % (save_path, validation_accuracy))\n",
    "print(\"OPTIMIZATION FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMPUTE TEST ACCURACY\n",
    "\n",
    "# RESTORE SAVED NETWORK\n",
    "saver.restore(sess, MODEL_DIRECTORY)\n",
    "\n",
    "# COMPUTE ACCURACY FOR TEST DATA\n",
    "test_size   = test_labels.shape[0]\n",
    "total_batch = int(test_size / batch_size)\n",
    "acc_buffer  = []\n",
    "for i in range(total_batch):\n",
    "    offset = (i * batch_size) % (test_size)\n",
    "    batch_xs = test_data[offset:(offset + batch_size), :]\n",
    "    batch_ys = test_labels[offset:(offset + batch_size), :]\n",
    "    y_final = sess.run(y, feed_dict={x: batch_xs, y_: batch_ys, is_training: False})\n",
    "    correct_prediction = np.equal(np.argmax(y_final, 1), np.argmax(batch_ys, 1))\n",
    "    acc_buffer.append(np.sum(correct_prediction.astype(float)) / batch_size)\n",
    "print(\"TEST ACCURACY IS: %.4f\" % np.mean(acc_buffer))\n",
    "\n",
    "# 테스트 결과\n",
    "# TEST ACCURACY IS: 0.9961"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
